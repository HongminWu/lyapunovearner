{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging \n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "from math import radians, cos, sin\n",
    "from scipy.cluster.vq import whiten, kmeans, kmeans2\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "disp=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the data we have collected\n",
    "with h5py.File('../scripts/{}.h5'.format('joints_data'), 'r+') as f:\n",
    "    pos_grp = f['workspace_coords']\n",
    "    q       = pos_grp['joint_positions'].value\n",
    "    qdot    = pos_grp['joint_velocities'].value\n",
    "    targ_grp = f['workspace_targets']\n",
    "    qstar   = targ_grp['joint_positions'].value\n",
    "    qdot_star= targ_grp['joint_velocities'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = q[0:7,:]\n",
    "q2 = q[7:14,:]\n",
    "q1dot = qdot[0:7,:]\n",
    "q2dot = qdot[7:14,:]\n",
    "pts = dict(pts1=dict(q=q1, qdot=q1dot),pts2=dict(q=q2, qdot=q2dot),targets=dict(q=qstar, qdot=qdot_star),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "if disp:\n",
    "    print(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert from joint space to cartesian coordinates in the end effector/tool frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### defining D-H constants\n",
    "\"\"\"lc, lf are in millimeters\"\"\"\n",
    "lc, lf = 300, 350\n",
    "\n",
    "pts_cartesian = {\n",
    "    'pts1': {\n",
    "        'x': None, 'y': None, 'z': None,\n",
    "        'xdot': None, 'ydot': None, 'zdot': None,\n",
    "    },\n",
    "    'pts2': {\n",
    "        'x': None, 'y': None, 'z': None,\n",
    "        'xdot': None, 'ydot': None, 'zdot': None,\n",
    "    },\n",
    "    'targets': {\n",
    "        'x': None, 'y': None, 'z': None,\n",
    "        'xdot': None, 'ydot': None, 'zdot': None,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['q', 'qdot'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts['pts1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "way_pts = [k for k, v in pts.items()]\n",
    "for way_pt in way_pts:\n",
    "    # get the positional points\n",
    "    px_l = [cos(x) for x in pts[way_pt]['q'][:,0]]\n",
    "    px_r1 = [lc*cos(x) for x in  pts[way_pt]['q'][:,1]]\n",
    "    px_r2 = [lf*cos(x+y) for x,y in  zip(pts[way_pt]['q'][:,1], pts[way_pt]['q'][:,2])]\n",
    "    px_r = np.array(px_r1) - px_r2\n",
    "    pts_cartesian[way_pt]['x'] = np.array(px_l) * np.array(px_r)\n",
    "\n",
    "    py_l = [sin(x) for x in pts[way_pt]['q'][:,0]]\n",
    "    pts_cartesian[way_pt]['y'] = np.array(py_l) * np.array(px_r)\n",
    "\n",
    "    pz_l = [-lc * sin(x) for x in pts[way_pt]['q'][:,1]]\n",
    "    pz_r = [lf * cos(x+y) for x, y in zip(pts[way_pt]['q'][:,1], pts[way_pt]['q'][:,2])]\n",
    "    pts_cartesian[way_pt]['z'] = np.array(pz_l) - np.array(pz_r)\n",
    "    \n",
    "    # get the velocities\n",
    "    px_l = [cos(x) for x in pts[way_pt]['qdot'][:,0]]\n",
    "    px_r1 = [lc*cos(x) for x in  pts[way_pt]['qdot'][:,1]]\n",
    "    px_r2 = [lf*cos(x+y) for x,y in  zip(pts[way_pt]['qdot'][:,1], pts[way_pt]['qdot'][:,2])]\n",
    "    px_r = np.array(px_r1) - px_r2\n",
    "    pts_cartesian[way_pt]['xdot'] = np.array(px_l) * np.array(px_r)\n",
    "\n",
    "    py_l = [sin(x) for x in pts[way_pt]['qdot'][:,0]]\n",
    "    pts_cartesian[way_pt]['ydot'] = np.array(py_l) * np.array(px_r)\n",
    "\n",
    "    pz_l = [-lc * sin(x) for x in pts[way_pt]['qdot'][:,1]]\n",
    "    pz_r = [lf * cos(x+y) for x, y in zip(pts[way_pt]['qdot'][:,1], pts[way_pt]['qdot'][:,2])]\n",
    "    pts_cartesian[way_pt]['zdot'] = np.array(pz_l) - np.array(pz_r)\n",
    "    \n",
    "# print(pts_cartesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gather all the cartesian points and velocities\n",
    "+ data is thus arranged:\n",
    "  - data = [\n",
    "             \\zeta^0, \\zeta^1, \\zeta^\\star; \n",
    "             \\dot{\\zeta^0}, \\dot{\\zeta^1}, \\dot{\\zeta^\\star}\n",
    "            ]\n",
    "  - where \\zeta \\in R^n, n being the dimension in cartesian coordinates of the \n",
    "  - note that our data is in 2D for now, i.e. x and y axes in Cartesian space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack([pts_cartesian['pts1']['x'], pts_cartesian['pts2']['x'], pts_cartesian['targets']['x']])\n",
    "y = np.hstack([pts_cartesian['pts1']['y'], pts_cartesian['pts2']['y'], pts_cartesian['targets']['y']])\n",
    "z = np.hstack([pts_cartesian['pts1']['z'], pts_cartesian['pts2']['z'], pts_cartesian['targets']['z']])\n",
    "\n",
    "# xd = np.hstack([pts_cartesian['pts1']['xdot'], pts_cartesian['pts2']['xdot'], pts_cartesian['targets']['xdot']])\n",
    "# yd = np.hstack([pts_cartesian['pts1']['ydot'], pts_cartesian['pts2']['ydot'], pts_cartesian['targets']['ydot']])\n",
    "# zd = np.hstack([pts_cartesian['pts1']['zdot'], pts_cartesian['pts2']['zdot'], pts_cartesian['targets']['zdot']])\n",
    "xd,yd,zd = [[] for _ in range(3)]\n",
    "for i in range(len(x)-1):\n",
    "    xd.append((x[i+1]-x[i])/0.1)\n",
    "    yd.append((y[i+1]-y[i])/0.1)\n",
    "    zd.append((z[i+1]-z[i])/0.1)\n",
    "xd.append(0)\n",
    "yd.append(0)\n",
    "zd.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape, y.shape, z.shape\n",
    "data = np.vstack([x,y,np.array(xd),np.array(yd)])\n",
    "#data = np.vstack([x,y,z])\n",
    "# print(data.shape)\n",
    "filename = '../scripts/{}.h5'.format('torobo_processed_data')\n",
    "os.remove(filename) if os.path.isfile(filename) else None\n",
    "# time.sleep(4)\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    pos_grp = f.create_group('data')\n",
    "    pos_grp.create_dataset(\"data\", data=data, dtype=np.float32, compression=\"gzip\", compression_opts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now compute the priors, mus and sigmas of the Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_init_data(data, num_states=2):\n",
    "    \"\"\"\n",
    "        num_states: number of k centroids\n",
    "    \"\"\"\n",
    "    whitened = whiten(data)\n",
    "    centroids, labels = kmeans2(whitened, num_states, iter=500, thresh=1e-05, check_finite=True)\n",
    "    \n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This file defines a Gaussian mixture model class. \"\"\"\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "def logsum(vec, axis=0, keepdims=True):\n",
    "    #TODO: Add a docstring.\n",
    "    maxv = np.max(vec, axis=axis, keepdims=keepdims)\n",
    "    maxv[maxv == -float('inf')] = 0\n",
    "    return np.log(np.sum(np.exp(vec-maxv), axis=axis, keepdims=keepdims)) + maxv\n",
    "\n",
    "def check_sigma(A):\n",
    "    \"\"\"\n",
    "        checks if the sigma matrix is symmetric\n",
    "        positive definite before inverting via cholesky decomposition\n",
    "    \"\"\"\n",
    "    eigval = np.linalg.eigh(A)[0]\n",
    "    if np.array_equal(A, A.T) and np.all(eigval>0):\n",
    "        # LOGGER.debug(\"sigma is pos. def. Computing cholesky factorization\")\n",
    "        return A\n",
    "    else:\n",
    "        # find lowest eigen value\n",
    "        eta = 1e-6  # regularizer for matrix multiplier\n",
    "        low = np.amin(np.sort(eigval))\n",
    "        Anew = low * A + eta * np.eye(A.shape[0])\n",
    "        return Anew\n",
    "\n",
    "class GMM(object):\n",
    "    \"\"\" Gaussian Mixture Model. \"\"\"\n",
    "    def __init__(self, init_sequential=False, eigreg=False, warmstart=True):\n",
    "        self.init_sequential = init_sequential\n",
    "        self.eigreg = eigreg\n",
    "        self.warmstart = warmstart\n",
    "        self.sigma = None\n",
    "\n",
    "    def inference(self, pts):\n",
    "        \"\"\"\n",
    "        Evaluate dynamics prior.\n",
    "        Args:\n",
    "            pts: A N x D array of points.\n",
    "        \"\"\"\n",
    "        # Compute posterior cluster weights.\n",
    "        logwts = self.clusterwts(pts)\n",
    "\n",
    "        # Compute posterior mean and covariance.\n",
    "        mu0, Phi = self.moments(logwts)\n",
    "\n",
    "        # Set hyperparameters.\n",
    "        m = self.N\n",
    "        n0 = m - 2 - mu0.shape[0]\n",
    "\n",
    "        # Normalize.\n",
    "        m = float(m) / self.N\n",
    "        n0 = float(n0) / self.N\n",
    "        return mu0, Phi, m, n0\n",
    "\n",
    "    def clusterwts(self, data):\n",
    "        \"\"\"\n",
    "        Compute cluster weights for specified points under GMM.\n",
    "        Args:\n",
    "            data: An N x D array of points\n",
    "        Returns:\n",
    "            A K x 1 array of average cluster log probabilities.\n",
    "        \"\"\"\n",
    "        # Compute probability of each point under each cluster.\n",
    "        logobs = self.estep(data)\n",
    "\n",
    "        # Renormalize to get cluster weights.\n",
    "        logwts = logobs - logsum(logobs, axis=1)\n",
    "\n",
    "        # Average the cluster probabilities.\n",
    "        logwts = logsum(logwts, axis=0) - np.log(data.shape[0])\n",
    "        return logwts.T\n",
    "\n",
    "    def estep(self, data):\n",
    "        \"\"\"\n",
    "        Compute log observation probabilities under GMM.\n",
    "        Args:\n",
    "            data: A N x D array of points.\n",
    "        Returns:\n",
    "            logobs: A N x K array of log probabilities (for each point\n",
    "                on each cluster).\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N, D = data.shape\n",
    "        K = self.sigma.shape[0]\n",
    "\n",
    "        logobs = -0.5*np.ones((N, K))*D*np.log(2*np.pi)\n",
    "        for i in range(K):\n",
    "            mu, sigma = self.mu[i], self.sigma[i]\n",
    "            sigma = sigma\n",
    "            L = scipy.linalg.cholesky(sigma, lower=True)\n",
    "            logobs[:, i] -= np.sum(np.log(np.diag(L)))\n",
    "            diff = (data - mu).T\n",
    "            soln = scipy.linalg.solve_triangular(L, diff, lower=True)\n",
    "            logobs[:, i] -= 0.5*np.sum(soln**2, axis=0)\n",
    "\n",
    "        logobs += self.logmass.T\n",
    "        return logobs\n",
    "\n",
    "    def moments(self, logwts):\n",
    "        \"\"\"\n",
    "            Compute the moments of the cluster mixture with logwts.\n",
    "            Args:\n",
    "                logwts: A K x 1 array of log cluster probabilities.\n",
    "            Returns:\n",
    "                mu: A (D,) mean vector.\n",
    "                sigma: A D x D covariance matrix.\n",
    "        \"\"\"\n",
    "        # Exponentiate.\n",
    "        wts = np.exp(logwts)\n",
    "\n",
    "        # Compute overall mean.\n",
    "        mu = np.sum(self.mu * wts, axis=0)\n",
    "\n",
    "        # Compute overall covariance.\n",
    "        diff = self.mu - np.expand_dims(mu, axis=0)\n",
    "        diff_expand = np.expand_dims(self.mu, axis=1) * \\\n",
    "                np.expand_dims(diff, axis=2)\n",
    "        wts_expand = np.expand_dims(wts, axis=2)\n",
    "        sigma = np.sum((self.sigma + diff_expand) * wts_expand, axis=0)\n",
    "        return mu, sigma\n",
    "\n",
    "    def update(self, data, K, max_iterations=100):\n",
    "        \"\"\"\n",
    "        Run EM to update clusters.\n",
    "        Args:\n",
    "            data: An N x D data matrix, where N = number of data points.\n",
    "            K: Number of clusters to use.\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N  = data.shape[0]\n",
    "        Do = data.shape[1]\n",
    "\n",
    "        LOGGER.debug('Fitting GMM with %d clusters on %d points.', K, N)\n",
    "\n",
    "        if (not self.warmstart or self.sigma is None or K != self.sigma.shape[0]):\n",
    "            # Initialization.\n",
    "            LOGGER.debug('Initializing GMM.')\n",
    "            self.sigma = np.zeros((K, Do, Do))\n",
    "            self.mu = np.zeros((K, Do))\n",
    "            self.logmass = np.log(1.0 / K) * np.ones((K, 1))\n",
    "            self.mass = (1.0 / K) * np.ones((K, 1))\n",
    "            self.N = data.shape[0]\n",
    "            N = self.N\n",
    "\n",
    "            # Set initial cluster indices.\n",
    "            if not self.init_sequential:\n",
    "                cidx = np.random.randint(0, K, size=(1, N))\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # Initialize.\n",
    "            for i in range(K):\n",
    "                cluster_idx = (cidx == i)[0]\n",
    "                mu = np.mean(data[cluster_idx, :], axis=0)\n",
    "                diff = (data[cluster_idx, :] - mu).T\n",
    "                sigma = (1.0 / K) * (diff.dot(diff.T))\n",
    "                self.mu[i, :] = mu\n",
    "                self.sigma[i, :, :] = sigma + np.eye(Do) * 2e-6\n",
    "\n",
    "        prevll = -float('inf')\n",
    "        for itr in range(max_iterations):\n",
    "            # E-step: compute cluster probabilities.\n",
    "            logobs = self.estep(data)\n",
    "\n",
    "            # Compute log-likelihood.\n",
    "            ll = np.sum(logsum(logobs, axis=1))\n",
    "            LOGGER.debug('GMM itr %d/%d. Log likelihood: %f',\n",
    "                         itr, max_iterations, ll)\n",
    "            if ll < prevll:\n",
    "                # TODO: Why does log-likelihood decrease sometimes?\n",
    "                LOGGER.debug('Log-likelihood decreased! Ending on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            if np.abs(ll-prevll) < 1e-5*prevll:\n",
    "                LOGGER.debug('GMM converged on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            prevll = ll\n",
    "\n",
    "            # Renormalize to get cluster weights.\n",
    "            logw = logobs - logsum(logobs, axis=1)\n",
    "            assert logw.shape == (N, K)\n",
    "\n",
    "            # Renormalize again to get weights for refitting clusters.\n",
    "            logwn = logw - logsum(logw, axis=0)\n",
    "            assert logwn.shape == (N, K)\n",
    "            w = np.exp(logwn)\n",
    "\n",
    "            # M-step: update clusters.\n",
    "            # Fit cluster mass.\n",
    "            self.logmass = logsum(logw, axis=0).T\n",
    "            self.logmass = self.logmass - logsum(self.logmass, axis=0)\n",
    "            assert self.logmass.shape == (K, 1)\n",
    "            self.mass = np.exp(self.logmass)\n",
    "\n",
    "            # Reboot small clusters.\n",
    "            w[:, (self.mass < (1.0 / K) * 1e-4)[:, 0]] = 1.0 / N\n",
    "            # Fit cluster means.\n",
    "            w_expand = np.expand_dims(w, axis=2)\n",
    "            data_expand = np.expand_dims(data, axis=1)\n",
    "            self.mu = np.sum(w_expand * data_expand, axis=0)\n",
    "            # Fit covariances.\n",
    "            wdata = data_expand * np.sqrt(w_expand)\n",
    "            assert wdata.shape == (N, K, Do)\n",
    "            for i in range(K):\n",
    "                # Compute weighted outer product.\n",
    "                XX = wdata[:, i, :].T.dot(wdata[:, i, :])\n",
    "                mu = self.mu[i, :]\n",
    "                self.sigma[i, :, :] = XX - np.outer(mu, mu)\n",
    "\n",
    "                if self.eigreg:  # Use eigenvalue regularization.\n",
    "                    raise NotImplementedError()\n",
    "                else:  # Use quick and dirty regularization.\n",
    "                    sigma = self.sigma[i, :, :]\n",
    "                    self.sigma[i, :, :] = 0.5 * (sigma + sigma.T) + \\\n",
    "                            1e-6 * np.eye(Do)\n",
    "                    \n",
    "gmm = GMM()\n",
    "gmm.update(data.T, K=6, max_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, priors = gmm.mu, gmm.sigma, gmm.logmass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4) (6, 4, 4) (6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(mu.shape, sigma.shape, priors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = data.shape[0]/2\n",
    "num_states = 4\n",
    "num_var, num_data = data.shape\n",
    "centers, labels = kmeans_init_data(data, num_states=num_states)\n",
    "mu = centers.T\n",
    "\n",
    "print('labels: ', labels)\n",
    "print('centers: ', centers)\n",
    "Priors = np.zeros((num_states))\n",
    "Sigma  = np.zeros((num_states, num_states, num_states))\n",
    "\n",
    "for i in range(num_states):\n",
    "    idtemp = np.nonzero(labels==i)\n",
    "    Priors[i] = len(idtemp[0])\n",
    "    print('Priors: ', Priors[i])\n",
    "    print('idtemp[0]: ', idtemp[0], 'labels[idtemp]: ', labels[idtemp])\n",
    "    if labels[idtemp].size == 0:\n",
    "        Sigma[:,:,i] = np.random.randn(num_states, num_states)\n",
    "    else:\n",
    "        Sigma[:,:,i] = np.cov(m=data[:, idtemp[0]])\n",
    "    print('data[:, idtemp[0]]: ', data[:, idtemp[0]].T)\n",
    "    print('Sigma[:,:,i]: \\n', Sigma[:,:,i])\n",
    "    print(1e-5 * np.diag(np.ones((num_var, 1))))\n",
    "    # avoid numerical stability\n",
    "    Sigma[:,:,i] = Sigma[:,:,i] + 1e-5 * np.diag(np.ones([num_var, 1]))\n",
    "\n",
    "Priors = np.divide(Priors, sum(Priors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
