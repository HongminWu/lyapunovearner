{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, sys, os\n",
    "import h5py\n",
    "import logging\n",
    "import scipy.linalg\n",
    "import scipy.linalg as LA\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "from os.path import join, expanduser\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "##LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gather all the cartesian points and velocities\n",
    "+ data is thus arranged:\n",
    "  - data = [\n",
    "             \\zeta^0, \\zeta^1, \\zeta^\\star; \n",
    "             \\dot{\\zeta^0}, \\dot{\\zeta^1}, \\dot{\\zeta^\\star}\n",
    "            ]\n",
    "  - where \\zeta \\in R^n, n being the dimension in cartesian coordinates of the \n",
    "  - note that our data is in 2D for now, i.e. x and y axes in Cartesian space\n",
    "  \n",
    "  In other words, data is thus shaped:\n",
    "                  \n",
    "  data = \n",
    "                  point p1                               point p2                          point p3                  target joint space angles\n",
    "         [x1,  x2,  x3,  x4,  x5,  x6,  x7 | x1,  x2,  x3,  x4,  x5,  x6,  x7 | x1,  x2,  x3,  x4,  x5,  x6,  x7 | x1,  x2,  x3,  x4,  x5,  x6,  x7 ]\n",
    "         [y1,  y2,  y3,  y4,  y5,  y6,  y7 | y1,  y2,  y3,  y4,  y5,  y6,  y7 | y1,  y2,  y3,  y4,  y5,  y6,  y7 | y1,  y2,  y3,  y4,  y5,  y6,  y7 ]\n",
    "         [x1d, x2d, x3d, x4d, x5d, x6d, x7d| x1d, x2d, x3d, x4d, x5d, x6d, x7d| x1d, x2d, x3d, x4d, x5d, x6d, x7d| x1d, x2d, x3d, x4d, x5d, x6d, x7d]\n",
    "         [y1d, y2d, y3d, y4d, y5d, y6d, y7d| y1d, y2d, y3d, y4d, y5d, y6d, y7d| y1d, y2d, y3d, y4d, y5d, y6d, y7d| y1d, y2d, y3d, y4d, y5d, y6d, y7d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 21)\n"
     ]
    }
   ],
   "source": [
    "filename = '../scripts/{}.h5'.format('torobo_processed_data')\n",
    "with h5py.File(filename, 'r+') as f:\n",
    "    data = f['data/data'].value\n",
    "print(data.shape)\n",
    "\n",
    "# gmm = GMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.9579880e+01 -4.9652107e+01 -4.9599106e+01 -4.9568935e+01\n",
      "  -4.9374817e+01 -4.9628201e+01 -4.9575825e+01 -1.7277985e+02\n",
      "  -1.7339091e+02 -1.7450108e+02 -1.7185472e+02 -1.7065906e+02\n",
      "  -1.7141064e+02 -1.7340883e+02]\n",
      " [ 1.4874008e-01  1.9860950e-01  1.9839750e-01 -0.0000000e+00\n",
      "   4.9374837e-02  9.9256538e-02  1.9830436e-01  8.6390638e-01\n",
      "  -8.6696178e-01 -0.0000000e+00 -6.8742263e-01  5.1197869e-01\n",
      "  -1.0284761e+00  6.9363904e-01]\n",
      " [-7.2230250e-01  5.3001994e-01  3.0171213e-01  1.9411738e+00\n",
      "  -2.5338330e+00  5.2378392e-01 -1.2320402e+03 -6.1107283e+00\n",
      "  -1.1101608e+01  2.6463499e+01  1.1956734e+01 -7.5158463e+00\n",
      "  -1.9981880e+01  1.7201422e+03]\n",
      " [ 4.9869421e-01 -2.1200913e-03 -1.9839749e+00  4.9374837e-01\n",
      "   4.9881703e-01  9.9047822e-01  6.6560202e+00 -1.7308681e+01\n",
      "   8.6696177e+00 -6.8742261e+00  1.1994013e+01 -1.5404549e+01\n",
      "   1.7221151e+01 -6.9084983e+00]]\n",
      "\n",
      "[[-1.3946031e+00 -6.3601512e-01 -1.1968470e+00 -2.7254500e+00\n",
      "  -9.0140814e-01 -9.6777433e-01 -2.1465166e+00]\n",
      " [ 2.7892101e-03 -3.1801020e-03 -9.5749805e-03 -1.6352898e-02\n",
      "  -4.5070783e-03 -6.7745312e-03  6.4395694e-03]\n",
      " [ 7.5858803e+00 -5.6083188e+00 -1.5286032e+01  1.8240419e+01\n",
      "  -6.6366196e-01 -1.1787422e+01  0.0000000e+00]\n",
      " [-5.9693120e-02 -6.3948788e-02 -6.7779161e-02  1.1845819e-01\n",
      "  -2.2674531e-02  1.3214101e-01  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x0 = data[:, :14]\n",
    "xT = data[:, 14:]\n",
    "print(x0)\n",
    "print()\n",
    "print(xT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now compute the priors, mus and sigmas of the Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_init_lyap(data, Vxf0, b_initRandom=False):\n",
    "    \"\"\"\n",
    "    This function guesses the initial lyapunov function\n",
    "    \"\"\"\n",
    "    # allocate spaces for incoming arrays\n",
    "    Vxf0['Mu']  =  np.zeros(( Vxf0['d'], Vxf0['L']+1 )) # will be 2x2\n",
    "    Vxf0['P']   =  np.zeros(( Vxf0['d'], Vxf0['d'], Vxf0['L']+1)) # wil be 2x2x3\n",
    "\n",
    "    if b_initRandom:\n",
    "        lengthScale = np.sqrt(np.var(data[:Vxf0['d'],:].T, axis=0))\n",
    "        lengthScale = np.ravel(lengthScale)\n",
    "        '''\n",
    "         If `rowvar` is True (default), then each row represents a\n",
    "        variable, with observations in the columns. Otherwise, the relationship\n",
    "        is transposed: each column represents a variable, while the rows\n",
    "        contain observations.\n",
    "        '''\n",
    "        #tempcov = np.cov(np.var(data[:Vxf0['d'],:], axis=0), rowvar=False)\n",
    "        lengthScaleMatrix = LA.sqrtm(np.cov(np.var(data[:Vxf0['d'],:], axis=0), rowvar=False))\n",
    "        Vxf0['Priors'] = np.random.rand(Vxf0['L']+1,1)\n",
    "\n",
    "        for l in range(Vxf0['L']+1):\n",
    "            tempMat = np.random.randn(Vxf0['d'], Vxf0['d'])\n",
    "            Vxf0['Mu'][:,l] = np.multiply(np.random.randn(Vxf0['d'],1), lengthScale)\n",
    "            Vxf0['P'][:,:,l] = lengthScaleMatrix.dot((tempMat * tempMat.T)).dot(lengthScaleMatrix)\n",
    "    else:\n",
    "        Vxf0['Priors'] = np.ones((Vxf0['L']+1, 1))\n",
    "        Vxf0['Priors'] = Vxf0['Priors']/np.sum(Vxf0['Priors'])\n",
    "        Vxf0['Mu'] = np.zeros((Vxf0['d'], Vxf0['L']+1))\n",
    "\n",
    "        Vxf0['P']   =  np.zeros(( Vxf0[ 'd'], Vxf0['d'], Vxf0['L']+1)) # wil be 2x2x3\n",
    "        for l in range(Vxf0['L']+1):\n",
    "            Vxf0['P'][:,:,l] = np.eye((Vxf0['d']))\n",
    "\n",
    "    Vxf0.update(Vxf0)\n",
    "\n",
    "    return Vxf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vxf0 = {\n",
    "    'L': None,\n",
    "    'd': None,\n",
    "    'w': 1e-4, #A positive scalar weight regulating the priority between the two objectives of the opitmization. Please refer to the page 7 of the paper for further information.\n",
    "    'Mu': np.array(()),\n",
    "    'P': np.array(()),\n",
    "}\n",
    "\n",
    "options = {\n",
    "    'tol_mat_bias': 1e-1,\n",
    "    'display': 1,\n",
    "    'tol_stopping': 1e-10,\n",
    "    'max_iter': 500,\n",
    "    'optimizePriors': True,\n",
    "    'upperBoundEigenValue': True,\n",
    "}\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'use_cvxopt': True, #whether to use cvxopt package, fmincon or otherwise\n",
    "}\n",
    "\n",
    "options = {\n",
    "    'tol_mat_bias': 1e-1,\n",
    "    'display': 1,\n",
    "    'tol_stopping': 1e-10,\n",
    "    'max_iter':500,\n",
    "    'optimizePriors': True,\n",
    "    'upperBoundEigenValue': True\n",
    "}\n",
    "\n",
    "\"\"\" This file defines a Gaussian mixture model class. \"\"\"\n",
    "\n",
    "def logsum(vec, axis=0, keepdims=True):\n",
    "    #TODO: Add a docstring.\n",
    "    maxv = np.max(vec, axis=axis, keepdims=keepdims)\n",
    "    maxv[maxv == -float('inf')] = 0\n",
    "    return np.log(np.sum(np.exp(vec-maxv), axis=axis, keepdims=keepdims)) + maxv\n",
    "\n",
    "def check_sigma(A):\n",
    "    \"\"\"\n",
    "        checks if the sigma matrix is symmetric\n",
    "        positive definite before inverting via cholesky decomposition\n",
    "    \"\"\"\n",
    "    eigval = np.linalg.eigh(A)[0]\n",
    "    if np.array_equal(A, A.T) and np.all(eigval>0):\n",
    "        # LOGGER.debug(\"sigma is pos. def. Computing cholesky factorization\")\n",
    "        return A\n",
    "    else:\n",
    "        # find lowest eigen value\n",
    "        eta = 1e-6  # regularizer for matrix multiplier\n",
    "        low = np.amin(np.sort(eigval))\n",
    "        Anew = low * A + eta * np.eye(A.shape[0])\n",
    "        return Anew\n",
    "\n",
    "class GMM(object):\n",
    "    \"\"\" Gaussian Mixture Model. \"\"\"\n",
    "    def __init__(self, init_sequential=False, eigreg=False, warmstart=True):\n",
    "        self.init_sequential = init_sequential\n",
    "        self.eigreg = eigreg\n",
    "        self.warmstart = warmstart\n",
    "        self.sigma = None\n",
    "\n",
    "    def inference(self, pts):\n",
    "        \"\"\"\n",
    "        Evaluate dynamics prior.\n",
    "        Args:\n",
    "            pts: A N x D array of points.\n",
    "        \"\"\"\n",
    "        # Compute posterior cluster weights.\n",
    "        logwts = self.clusterwts(pts)\n",
    "\n",
    "        # Compute posterior mean and covariance.\n",
    "        mu0, Phi = self.moments(logwts)\n",
    "\n",
    "        # Set hyperparameters.\n",
    "        m = self.N\n",
    "        n0 = m - 2 - mu0.shape[0]\n",
    "\n",
    "        # Normalize.\n",
    "        m = float(m) / self.N\n",
    "        n0 = float(n0) / self.N\n",
    "        return mu0, Phi, m, n0\n",
    "\n",
    "    def clusterwts(self, data):\n",
    "        \"\"\"\n",
    "        Compute cluster weights for specified points under GMM.\n",
    "        Args:\n",
    "            data: An N x D array of points\n",
    "        Returns:\n",
    "            A K x 1 array of average cluster log probabilities.\n",
    "        \"\"\"\n",
    "        # Compute probability of each point under each cluster.\n",
    "        logobs = self.estep(data)\n",
    "\n",
    "        # Renormalize to get cluster weights.\n",
    "        logwts = logobs - logsum(logobs, axis=1)\n",
    "\n",
    "        # Average the cluster probabilities.\n",
    "        logwts = logsum(logwts, axis=0) - np.log(data.shape[0])\n",
    "        return logwts.T\n",
    "\n",
    "    def estep(self, data):\n",
    "        \"\"\"\n",
    "        Compute log observation probabilities under GMM.\n",
    "        Args:\n",
    "            data: A N x D array of points.\n",
    "        Returns:\n",
    "            logobs: A N x K array of log probabilities (for each point\n",
    "                on each cluster).\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N, D = data.shape\n",
    "        K = self.sigma.shape[0]\n",
    "\n",
    "        logobs = -0.5*np.ones((N, K))*D*np.log(2*np.pi)\n",
    "        for i in range(K):\n",
    "            mu, sigma = self.mu[i], self.sigma[i]\n",
    "            sigma = sigma\n",
    "            L = scipy.linalg.cholesky(sigma, lower=True)\n",
    "            logobs[:, i] -= np.sum(np.log(np.diag(L)))\n",
    "            diff = (data - mu).T\n",
    "            soln = scipy.linalg.solve_triangular(L, diff, lower=True)\n",
    "            logobs[:, i] -= 0.5*np.sum(soln**2, axis=0)\n",
    "\n",
    "        logobs += self.logmass.T\n",
    "        return logobs\n",
    "\n",
    "    def moments(self, logwts):\n",
    "        \"\"\"\n",
    "            Compute the moments of the cluster mixture with logwts.\n",
    "            Args:\n",
    "                logwts: A K x 1 array of log cluster probabilities.\n",
    "            Returns:\n",
    "                mu: A (D,) mean vector.\n",
    "                sigma: A D x D covariance matrix.\n",
    "        \"\"\"\n",
    "        # Exponentiate.\n",
    "        wts = np.exp(logwts)\n",
    "\n",
    "        # Compute overall mean.\n",
    "        mu = np.sum(self.mu * wts, axis=0)\n",
    "\n",
    "        # Compute overall covariance.\n",
    "        diff = self.mu - np.expand_dims(mu, axis=0)\n",
    "        diff_expand = np.expand_dims(self.mu, axis=1) * \\\n",
    "                np.expand_dims(diff, axis=2)\n",
    "        wts_expand = np.expand_dims(wts, axis=2)\n",
    "        sigma = np.sum((self.sigma + diff_expand) * wts_expand, axis=0)\n",
    "        return mu, sigma\n",
    "\n",
    "    def update(self, data, K, max_iterations=100):\n",
    "        \"\"\"\n",
    "        Run EM to update clusters.\n",
    "        Args:\n",
    "            data: An N x D data matrix, where N = number of data points.\n",
    "            K: Number of clusters to use.\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N  = data.shape[0]\n",
    "        Do = data.shape[1]\n",
    "\n",
    "        LOGGER.debug('Fitting GMM with %d clusters on %d points.', K, N)\n",
    "\n",
    "        if (not self.warmstart or self.sigma is None or K != self.sigma.shape[0]):\n",
    "            # Initialization.\n",
    "            LOGGER.debug('Initializing GMM.')\n",
    "            self.sigma = np.zeros((K, Do, Do))\n",
    "            self.mu = np.zeros((K, Do))\n",
    "            self.logmass = np.log(1.0 / K) * np.ones((K, 1))\n",
    "            self.mass = (1.0 / K) * np.ones((K, 1))\n",
    "            self.N = data.shape[0]\n",
    "            N = self.N\n",
    "\n",
    "            # Set initial cluster indices.\n",
    "            if not self.init_sequential:\n",
    "                cidx = np.random.randint(0, K, size=(1, N))\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # Initialize.\n",
    "            for i in range(K):\n",
    "                cluster_idx = (cidx == i)[0]\n",
    "                mu = np.mean(data[cluster_idx, :], axis=0)\n",
    "                diff = (data[cluster_idx, :] - mu).T\n",
    "                sigma = (1.0 / K) * (diff.dot(diff.T))\n",
    "                self.mu[i, :] = mu\n",
    "                self.sigma[i, :, :] = sigma + np.eye(Do) * 2e-6\n",
    "\n",
    "        prevll = -float('inf')\n",
    "        for itr in range(max_iterations):\n",
    "            # E-step: compute cluster probabilities.\n",
    "            logobs = self.estep(data)\n",
    "\n",
    "            # Compute log-likelihood.\n",
    "            ll = np.sum(logsum(logobs, axis=1))\n",
    "            LOGGER.debug('GMM itr %d/%d. Log likelihood: %f',\n",
    "                         itr, max_iterations, ll)\n",
    "            if ll < prevll:\n",
    "                # TODO: Why does log-likelihood decrease sometimes?\n",
    "                LOGGER.debug('Log-likelihood decreased! Ending on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            if np.abs(ll-prevll) < 1e-5*prevll:\n",
    "                LOGGER.debug('GMM converged on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            prevll = ll\n",
    "\n",
    "            # Renormalize to get cluster weights.\n",
    "            logw = logobs - logsum(logobs, axis=1)\n",
    "            assert logw.shape == (N, K)\n",
    "\n",
    "            # Renormalize again to get weights for refitting clusters.\n",
    "            logwn = logw - logsum(logw, axis=0)\n",
    "            assert logwn.shape == (N, K)\n",
    "            w = np.exp(logwn)\n",
    "\n",
    "            # M-step: update clusters.\n",
    "            # Fit cluster mass.\n",
    "            self.logmass = logsum(logw, axis=0).T\n",
    "            self.logmass = self.logmass - logsum(self.logmass, axis=0)\n",
    "            assert self.logmass.shape == (K, 1)\n",
    "            self.mass = np.exp(self.logmass)\n",
    "\n",
    "            # Reboot small clusters.\n",
    "            w[:, (self.mass < (1.0 / K) * 1e-4)[:, 0]] = 1.0 / N\n",
    "            # Fit cluster means.\n",
    "            w_expand = np.expand_dims(w, axis=2)\n",
    "            data_expand = np.expand_dims(data, axis=1)\n",
    "            self.mu = np.sum(w_expand * data_expand, axis=0)\n",
    "            # Fit covariances.\n",
    "            wdata = data_expand * np.sqrt(w_expand)\n",
    "            assert wdata.shape == (N, K, Do)\n",
    "            for i in range(K):\n",
    "                # Compute weighted outer product.\n",
    "                XX = wdata[:, i, :].T.dot(wdata[:, i, :])\n",
    "                mu = self.mu[i, :]\n",
    "                self.sigma[i, :, :] = XX - np.outer(mu, mu)\n",
    "\n",
    "                if self.eigreg:  # Use eigenvalue regularization.\n",
    "                    raise NotImplementedError()\n",
    "                else:  # Use quick and dirty regularization.\n",
    "                    sigma = self.sigma[i, :, :]\n",
    "                    self.sigma[i, :, :] = 0.5 * (sigma + sigma.T) + \\\n",
    "                            1e-6 * np.eye(Do)\n",
    "                    \n",
    "def gaussPDF(data, mu, sigma):\n",
    "    nbVar, nbdata = data.shape\n",
    "\n",
    "    data = data.T - np.tile(mu.T, [nbdata,1])\n",
    "    prob = np.sum((data/sigma)*data, axis=1);\n",
    "    prob = np.exp(-0.5*prob) / np.sqrt((2*np.pi)**nbVar *\n",
    "                                       (np.abs(np.linalg.det(sigma))+\n",
    "                                        np.finifo(np.float64).min))\n",
    "    return prob\n",
    "\n",
    "def GMR(Priors, Mu, Sigma, x, inp, out, nargout=3):\n",
    "    nbData = x.shape[1]\n",
    "    nbVar = Mu.shape[0]\n",
    "    nbStates = Sigma.shape[2]\n",
    "\n",
    "    Pxi = np.zeros_like(Priors)\n",
    "    for i in range(nbStates):\n",
    "        Pxi[:,i] = Priors[i] * gaussPDF(x, Mu[inp,i], Sigma[inp,inp,i])\n",
    "\n",
    "    beta = Pxi / np.tile(np.sum(Pxi,axis=1) +\n",
    "                         np.finfo(np.float32).min, [1,nbStates])\n",
    "    #########################################################################\n",
    "    for j in range(nbStates):\n",
    "        y_tmp[:,:,j] = np.tile(Mu[out,j],[1,nbData]) \\\n",
    "                     + Sigma[out,inp,j]/(Sigma[inp,inp,j]).dot(x-np.tile(Mu[inp,j],[1,nbData]))\n",
    "\n",
    "    beta_tmp = beta.reshape(1, beta.shape)\n",
    "    y_tmp2 = np.tile(beta_tmp,[matlength(out), 1, 1]) * y_tmp\n",
    "    y = np.sum(y_tmp2,axis=2)\n",
    "    ## Compute expected covariance matrices Sigma_y, given input x\n",
    "    #########################################################################\n",
    "    if nargout > 1:\n",
    "        for j in range(nbStates):\n",
    "            Sigma_y_tmp[:,:,0,j] = Sigma[out,out,j] \\\n",
    "                                   - (Sigma[out,inp,j]/(Sigma[inp,inp,j])  \\\n",
    "                                   * Sigma[inp,out,j])\n",
    "\n",
    "        beta_tmp = beta.reshape(1, 1, beta.shape)\n",
    "        Sigma_y_tmp2 = np.tile(beta_tmp * beta_tmp, \\\n",
    "                               [matlength(out), matlength(out), 1, 1]) * \\\n",
    "                                np.tile(Sigma_y_tmp,[1, 1, nbData, 1])\n",
    "        Sigma_y = np.sum(Sigma_y_tmp2, axis=3)\n",
    "\n",
    "    return y, Sigma_y, beta\n",
    "\n",
    "def obj(p, x, xd, d, L, w, options):\n",
    "    Vxf         = shape_DS(p,d,L,options)\n",
    "    _, Vx       = computeEnergy(x, None, Vxf)\n",
    "    Vdot        = np.sum(Vx*xd, axis=0)  #derivative of J w.r.t. xd\n",
    "    norm_Vx     = np.sqrt(np.sum(Vx * Vx, axis=0))\n",
    "    norm_xd     = np.sqrt(np.sum(xd * xd, axis=0))\n",
    "    J           = np.divide(Vdot, np.multiply(norm_Vx, norm_xd))#.squeeze()\n",
    "\n",
    "    # projections onto positive orthant\n",
    "    J[np.where(norm_xd==0)] = 0\n",
    "    J[np.where(norm_Vx==0)] = 0\n",
    "    J[np.where(Vdot>0)]     = J[np.where(Vdot>0)]**2      # solves psi(t,n)**2\n",
    "    J[np.where(Vdot<0)]     = -w*J[np.where(Vdot<0)]**2   # # J should be (1, 750)\n",
    "    J                       = np.sum(J)\n",
    "    dJ                      = None\n",
    "    \n",
    "    return J#, dJ\n",
    "\n",
    "def optimize(obj_handle, p0):\n",
    "    opt = minimize(\n",
    "        obj_handle,\n",
    "        x0=p0,\n",
    "        method='L-BFGS-B',\n",
    "        jac=False,\n",
    "        bounds=[(0.0, None) for _ in range(len(p0))], # no negative p values\n",
    "        #bounds = Bounds(ctr_handle(p0), keep_feasible=True), # will produce c, ceq as lb and ub\n",
    "        options={'ftol': 1e-4, 'disp': True}\n",
    "        )\n",
    "    return opt\n",
    "        \n",
    "def gmm_2_parameters(Vxf, options):\n",
    "    # transforming optimization parameters into a column vector\n",
    "    d = Vxf['d']\n",
    "    if Vxf['L'] > 0:\n",
    "        if options['optimizePriors']:\n",
    "            p0 = np.vstack((\n",
    "                           np.expand_dims(np.ravel(Vxf['Priors']), axis=1), # will be a x 1\n",
    "                           np.expand_dims(Vxf['Mu'][:,1:], axis=1).reshape(Vxf['L']*d,1)\n",
    "                        ))\n",
    "        else:\n",
    "            p0 = Vxf['Mu'][:,2:].reshape(Vxf['L']*d, 1) #print(p0) # p0 will be 4x1\n",
    "    else:\n",
    "        p0 = []\n",
    "\n",
    "    for k in range(Vxf['L']):\n",
    "        p0 = np.vstack((\n",
    "                      p0,\n",
    "                      Vxf['P'][:,:,k+1].reshape(d**2,1)\n",
    "                    ))\n",
    "    return p0\n",
    "\n",
    "def parameters_2_gmm(popt, d, L, options):\n",
    "    # transforming the column of parameters into Priors, Mu, and P\n",
    "    Vxf = shape_DS(popt, d, L, options)\n",
    "\n",
    "    return Vxf\n",
    "\n",
    "def shape_DS(p,d,L,options):\n",
    "    # transforming the column of parameters into Priors, Mu, and P\n",
    "    P = np.zeros((d,d,L+1))\n",
    "    optimizePriors = options['optimizePriors']\n",
    "    if L == 0:\n",
    "        Priors = 1\n",
    "        Mu = np.zeros((d,1))\n",
    "        i_c = 1\n",
    "    else:\n",
    "        if options['optimizePriors']:\n",
    "            Priors = p[:L+1]\n",
    "            i_c = L+1\n",
    "        else:\n",
    "            Priors = np.ones((L+1,1))\n",
    "            i_c = 0\n",
    "\n",
    "        Priors = np.divide(Priors, np.sum(Priors))\n",
    "        Mu = np.hstack((np.zeros((d,1)), p[[i_c+ x for x in range(d*L)]].reshape(d,L)))\n",
    "        i_c = i_c+d*L+1\n",
    "\n",
    "    for k in range(L):\n",
    "        P[:,:,k+1] = p[range(i_c+k*(d**2)-1,i_c+(k+1)*(d**2)-1)].reshape(d,d)\n",
    "\n",
    "    Vxf           = dict()\n",
    "    Vxf['Priors'] = Priors\n",
    "    Vxf['Mu']     = Mu\n",
    "    Vxf['P']      = P\n",
    "    Vxf['SOS']    = 0\n",
    "    \n",
    "    return Vxf\n",
    "\n",
    "def matVecNorm(x):\n",
    "    return np.sqrt(np.sum(x**2, axis=0))\n",
    "\n",
    "def matlength(x):\n",
    "  # find the max of a numpy matrix dims\n",
    "  return np.max(x.shape)\n",
    "\n",
    "def ctr_eigenvalue(p,d,L,options):\n",
    "    Vxf = dict()\n",
    "    if L == -1: # SOS\n",
    "        Vxf['d'] = d\n",
    "        Vxf['n'] = int(np.sqrt(matlength(p)/d**2))\n",
    "        Vxf['P'] = p.reshape(Vxf['n']*d,Vxf['n']*d)\n",
    "        Vxf['SOS'] = 1\n",
    "        c  = np.zeros((Vxf['n']*d))\n",
    "        ceq = []\n",
    "    else:\n",
    "        Vxf = shape_DS(p,d,L,options)\n",
    "        if L > 0:\n",
    "            c  = np.zeros([(L+1)*d+(L+1)*options['optimizePriors']])  #+options.variableSwitch\n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq = np.zeros((L+1))\n",
    "            else:\n",
    "                ceq = [] \n",
    "        else:\n",
    "            c  = np.zeros((d))\n",
    "            ceq = Vxf['P'].T.ravel().dot(Vxf['P'].ravel()) - 2\n",
    "\n",
    "    dc = [] \n",
    "    dceq = [] \n",
    "\n",
    "    if L == -1:  # SOS\n",
    "        c = -LA.eigvals(Vxf['P'] + Vxf['P'].T - np.eye(Vxf['n']*d)*options['tol_mat_bias'])\n",
    "    else:\n",
    "        for k in range(L):\n",
    "            lambder = LA.eigvals(Vxf['P'][:,:,k+1] + (Vxf['P'][:,:,k+1]).T)/2.0\n",
    "            c[k*d:(k+1)*d] = -lambder.real + options['tol_mat_bias']\n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq[k+1] = 1.0 - np.sum(lambder.real) \n",
    "\n",
    "    if L > 0 and options['optimizePriors']:\n",
    "        c[(L+1)*d:(L+1)*d+L+1] = -Vxf['Priors'].squeeze()\n",
    "\n",
    "    return c, ceq \n",
    "\n",
    "def learnEnergy(Vxf0, Data, options):\n",
    "    d = int(Data.shape[0]/2)  \n",
    "    x = Data[:d,:]     \n",
    "    xd = Data[d:2*d,:]  \n",
    "    Vxf0['SOS'] = False\n",
    "    \n",
    "    # Transform the Lyapunov model to a vector of optimization parameters\n",
    "    if Vxf0['SOS']:\n",
    "        p0 = npr.randn(d*Vxf0['n'], d*Vxf0['n']);\n",
    "        p0 = p0.dot(p0.T)\n",
    "        p0 = np.ravel(p0)\n",
    "        Vxf0['L'] = -1; # to distinguish sos from other methods\n",
    "    else:\n",
    "        for l in range(Vxf0['L']):\n",
    "            try:\n",
    "                Vxf0['P'][:,:,l+1] = scipy.linalg.solve(Vxf0['P'][:,:,l+1], np.eye(d))\n",
    "                #print('Vxf0[:,:,l+1]: ', Vxf0['P'][:,:,l+1])\n",
    "            except LA.LinAlgError as e:\n",
    "                LOGGER.debug('LinAlgError: %s', e)\n",
    "\n",
    "        # in order to set the first component to be the closest Gaussian to origin\n",
    "        to_sort = matVecNorm(Vxf0['Mu'])\n",
    "        idx = np.argsort(to_sort, kind='mergesort')\n",
    "        Vxf0['Mu'] = Vxf0['Mu'][:,idx]\n",
    "        Vxf0['P']  = Vxf0['P'][:,:,idx]\n",
    "        p0 = gmm_2_parameters(Vxf0,options)\n",
    "\n",
    "    obj_handle = lambda p: obj(p, x, xd, d, Vxf0['L'], Vxf0['w'], options)\n",
    "    ctr_handle = lambda p: ctr_eigenvalue(p, d, Vxf0['L'], options)\n",
    "    \n",
    "    optim_res = optimize(obj_handle, p0) \n",
    "    popt, J = optim_res.x, optim_res.fun\n",
    "\n",
    "    if Vxf0['SOS']:\n",
    "        Vxf['d']    = d\n",
    "        Vxf['n']    = Vxf0['n']\n",
    "        Vxf['P']    = popt.reshape(Vxf['n']*d,Vxf['n']*d)\n",
    "        Vxf['SOS']  = 1\n",
    "        Vxf['p0']   = compute_Energy(zeros(d,1),[],Vxf)\n",
    "        #check_constraints(popt,ctr_handle,d,0,options)\n",
    "    else:\n",
    "        # transforming back the optimization parameters into the GMM model\n",
    "        Vxf             = parameters_2_gmm(popt,d,Vxf0['L'],options)\n",
    "        Vxf['Mu'][:,0]  = 0\n",
    "        Vxf['L']        = Vxf0['L']\n",
    "        Vxf['d']        = Vxf0['d']\n",
    "        Vxf['w']        = Vxf0['w']\n",
    "        #check_constraints(popt,ctr_handle,d,Vxf['L'],options)\n",
    "\n",
    "    sumDet = 0\n",
    "    for l in range(Vxf['L']+1):\n",
    "        sumDet += np.linalg.det(Vxf['P'][:,:,l])\n",
    "\n",
    "    Vxf['P'][:,:,0] = Vxf['P'][:,:,0]/sumDet\n",
    "    Vxf['P'][:,:,1:] = Vxf['P'][:,:,1:]/np.sqrt(sumDet)\n",
    "\n",
    "    return Vxf, J\n",
    "\n",
    "def computeEnergy(X,Xd,Vxf, nargout=2):\n",
    "    d = X.shape[0]\n",
    "    nDemo = 1 \n",
    "    if nDemo>1:\n",
    "        X = X.reshape(d,-1)\n",
    "        Xd = Xd.reshape(d,-1)\n",
    "\n",
    "    if Vxf['SOS']:\n",
    "        V, dV = sos_lyapunov(X, Vxf['P'], Vxf['d'], Vxf['n'])\n",
    "        if 'p0' in Vxf:\n",
    "            V -= Vxf['p0']\n",
    "    else:\n",
    "        V, dV = gmr_lyapunov(X, Vxf['Priors'], Vxf['Mu'], Vxf['P'])\n",
    "\n",
    "    if nargout > 1:\n",
    "        if not Xd:\n",
    "            Vdot = dV\n",
    "        else:\n",
    "            Vdot = np.sum(Xd*dV, axis=0)\n",
    "    if nDemo>1:\n",
    "        V = V.reshape(-1, nDemo).T\n",
    "        if nargout > 1:\n",
    "            Vdot = Vdot.reshape(-1, nDemo).T\n",
    "\n",
    "    return V, Vdot\n",
    "\n",
    "\n",
    "def gmr_lyapunov(x, Priors, Mu, P):\n",
    "    # print('x.shape: ', x.shape)\n",
    "    nbData = x.shape[1]\n",
    "    d = x.shape[0]\n",
    "    L = P.shape[2]-1;\n",
    "\n",
    "    # Compute the influence of each GMM component, given input x\n",
    "    for k in range(L):\n",
    "        P_cur               = P[:,:,k+1]\n",
    "        if k                == 0:\n",
    "            V_k             = np.sum(x * (P_cur.dot(x)), axis=0)\n",
    "            V               = Priors[k+1]*(V_k)\n",
    "            Vx              = Priors[k+1]*((P_cur+P_cur.T).dot(x))\n",
    "        else:\n",
    "            x_tmp           = x - np.tile(Mu[:,k+1], [nbData, 1]).T\n",
    "            V_k             = np.sum(P_cur.dot(x_tmp)*x, axis=0)\n",
    "            V_k[V_k < 0]    = 0\n",
    "            V              += Priors[k+1] * (V_k ** 2)\n",
    "            temp            = (2 * Priors[k+1]) * (V_k)\n",
    "            Vx              = Vx + np.tile(temp, [d,1])*(P_cur.dot(x_tmp) + P_cur.T.dot(x))\n",
    "    \n",
    "    return V, Vx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsStabilizer(X, fn_handle, Vxf, rho0, kappa0):\n",
    "    d = Vxf['d']\n",
    "    if X.shape[0] == 2*d:\n",
    "        Xd     = X[d:2*d,:]\n",
    "        X      = X[:d,:]\n",
    "    else:\n",
    "        if (len(getfullargspec(fn_handle).args) == 1):\n",
    "            Xd, _, _ = fn_handle(X)\n",
    "        elif (len(getfullargspec(fn_handle).args) == 2):\n",
    "            t  = X[d,:]\n",
    "            X  = X[d:]\n",
    "            Xd, _, _ = fn_handle(t,X)\n",
    "        else:\n",
    "            logger.CRITICAL('Unknown function handle!')\n",
    "\n",
    "    V,Vx    = computeEnergy(X,[],Vxf)\n",
    "    norm_Vx = np.sum(V ** 2, axis=0)\n",
    "    norm_x  = np.sum(X ** 2,axis=0)\n",
    "    \n",
    "    Vdot    = np.sum(Vx * Xd,axis=0)\n",
    "    rho     = rho0 * (1-np.exp(-kappa0*norm_x)) * np.sqrt(norm_Vx)\n",
    "    ind     = (Vdot + rho) >= 0\n",
    "    #ind     = np.where((Vdot + rho) >= 0)\n",
    "    u       = Xd * 0\n",
    "    \n",
    "    print(np.sum(ind))\n",
    "    if np.sum(ind)>0:\n",
    "        lambder   = (Vdot[ind] + rho[ind]) / norm_Vx[ind]\n",
    "        u[:,ind]  = -np.tile(lambder,[d,1]) * Vx[:,ind]\n",
    "        Xd[:,ind] = Xd[:,ind] + u[:,ind]\n",
    "\n",
    "#     if args:\n",
    "#         dt = args[0]\n",
    "#         xn = X + np.dot(Xd, dt)\n",
    "#         Vn = computeEnergy(xn,[],Vxf)\n",
    "#         ind = (Vn >= V)\n",
    "#         i = 0\n",
    "\n",
    "#         while(np.any(ind) and i < 10):\n",
    "#             alpha = np.divide(V[ind], Vn[ind])\n",
    "#             Xd[:,ind] = np.tile(alpha,[d,1]) * Xd[:,ind] - \\\n",
    "#                         np.tile(alpha * np.sum(Xd[:,ind] * \\\n",
    "#                         Vx[:,ind], axis=0)/norm_Vx[ind],[d,1])*Vx[:,ind]\n",
    "#             xn = X + np.dot(Xd,dt)\n",
    "#             Vn = computeEnergy(xn,np.array(()),Vxf)\n",
    "#             ind = (Vn >= V)\n",
    "#             i = i + 1\n",
    "\n",
    "    return Xd, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olalekan/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:292: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/olalekan/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:473: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gmm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9a42d4a09b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# print('J: ', J)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# print('\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogmass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gmm' is not defined"
     ]
    }
   ],
   "source": [
    "Vxf0['L'] = 2   # number of asymmetric quadratic components for L >= 0\n",
    "Vxf0['d'] = int(data.shape[0]/2)\n",
    "Vxf0.update(Vxf0)\n",
    "# A set of options that will be passed to the solver\n",
    "options = {\n",
    "    'tol_mat_bias': 1e-1,\n",
    "    'display': 1,\n",
    "    'tol_stopping': 1e-10,\n",
    "    'max_iter':500,\n",
    "    'optimizePriors': True,\n",
    "    'upperBoundEigenValue': True\n",
    "}\n",
    "\n",
    "Vxf0 = guess_init_lyap(data, Vxf0, b_initRandom=False)\n",
    "# for k, v in Vxf0.items():\n",
    "#     print(k, v)\n",
    "# print('\\n')\n",
    "Vxf, J = learnEnergy(Vxf0, data, options)\n",
    "# for k, v in Vxf.items():\n",
    "#     print(k, v)\n",
    "# print('J: ', J)`\n",
    "# print('\\n')\n",
    "gmm.update(data.T, K=6, max_iterations=100)\n",
    "mu, sigma, priors = gmm.mu, gmm.sigma, gmm.logmass\n",
    "\n",
    "print(mu.shape, sigma.shape, priors.shape)\n",
    "inp = range(0, Vxf['d'])\n",
    "out = range(Vxf['d'], 2* Vxf['d'])\n",
    "gmr_handle = lambda x: GMR(priors, mu, sigma, x, inp, out)\n",
    "rho0, kappa0 = 1.0, 1.0\n",
    "Xd, u = dsStabilizer(data, gmr_handle, Vxf, rho0, kappa0)\n",
    "print(Xd.shape, u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctr_eigenvalue(p,d,L,options):\n",
    "    Vxf = dict()\n",
    "    if L == -1: # SOS\n",
    "        Vxf['d'] = d\n",
    "        Vxf['n'] = int(np.sqrt(matlength(p)/d**2))\n",
    "        Vxf['P'] = p.reshape(Vxf['n']*d,Vxf['n']*d)\n",
    "        Vxf['SOS'] = 1\n",
    "        c  = np.zeros(( Vxf['n']*d, 1 ))\n",
    "        ceq = []\n",
    "    else:\n",
    "        Vxf = shape_DS(p,d,L,options)\n",
    "        if L > 0:\n",
    "            c  = np.zeros(((L+1)*d+(L+1)*options['optimizePriors'],1)) \n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq = np.zeros((L+1,1))\n",
    "            else:\n",
    "                ceq = []\n",
    "        else:\n",
    "            c  = np.zeros((d,1))\n",
    "            ceq = (np.ravel(Vxf['P']).T).dot(np.ravel(Vxf['P'])) -2\n",
    "\n",
    "    dc, dceq = [], []\n",
    "\n",
    "    if L == -1:  # SOS\n",
    "        c = -np.linalg.eigvals(Vxf['P'] + Vxf['P'].T - np.eye(Vxf['n']*d)*options['tol_mat_bias'])\n",
    "    else:\n",
    "        for k in range(L):\n",
    "            lambder = sp.linalg.eigvals(Vxf['P'][:,:,k+1] + (Vxf['P'][:,:,k+1]).T).real/2.0\n",
    "            lambder = np.expand_dims(lambder, axis=1)\n",
    "            c[k*d:(k+1)*d] = -lambder + options['tol_mat_bias']\n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq[k+1] = 1.0 - np.sum(lambder.real) # + Vxf.P(:,:,k+1)'\n",
    "\n",
    "        if L > 0 and options['optimizePriors']:\n",
    "            c[(L+1)*d:(L+1)*d+L+1] = -Vxf['Priors']\n",
    "\n",
    "    return c, ceq, dc, dceq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ind = np.array([ True,  True , True , True , True , True  ,True  ,True ,False , True ,False,\n",
    " False  ,True,  True , True,  True  ,True , True,  True,  True])\n",
    "print(ind.shape)\n",
    "print(np.sum(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tampy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8791b61af367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Lekan/catkin_ws/src/torobo/tampy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Lekan/catkin_ws/src/dp_planning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtampy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTampy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtampy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mORDER_SERVO_ON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mORDER_SERVO_OFF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mORDER_RUN_MODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCTRL_MODE_CURRENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tampy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/Lekan/catkin_ws/src/torobo/tampy')\n",
    "sys.path.append('/Users/Lekan/catkin_ws/src/dp_planning')\n",
    "from tampy.tampy import Tampy\n",
    "from tampy.tampy import ORDER_SERVO_ON, ORDER_SERVO_OFF, ORDER_RUN_MODE, CTRL_MODE_CURRENT\n",
    "\n",
    "class ToroboEnvironment(object):\n",
    "    def __init__(self, home_pos):\n",
    "        \"\"\"\n",
    "         in update(self, currents) currents are inindividual currents to each joint\n",
    "         According to Ryo-san, the max current is 0.5A and you should start with 0.05A or sth and gradually increase\n",
    "        \"\"\"\n",
    "        self.tampy = Tampy()\n",
    "        self.home_pos = home_pos\n",
    "        self.control_freq = 30.0\n",
    "        self.latest_control = time.time()\n",
    "\n",
    "    def set_position(self, positions):\n",
    "        self.tampy.move_to(positions)\n",
    "\n",
    "    def update(self, currents):\n",
    "        self.tampy.send_currents(currents)\n",
    "        time.sleep(max(\n",
    "            self.latest_control + 1.0 / self.control_freq - time.time(),\n",
    "            0\n",
    "        ))\n",
    "        self.latest_control = time.time()\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        rx = self.tampy.get_latest_rx()\n",
    "        positions = [j.position for j in rx.joints]\n",
    "        velocities = [j.velocity for j in rx.joints]\n",
    "        return positions, velocities\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.set_position(self.home_pos)\n",
    "        self.tampy.send(ORDER_RUN_MODE, value1=CTRL_MODE_CURRENT)\n",
    "        self.tampy.send(ORDER_SERVO_ON)\n",
    "        self.latest_control = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.tampy.send_currents([0] * 7)\n",
    "        # TODO: why do we need multiple calls to kill?\n",
    "        for _ in range(3):\n",
    "            self.tampy.send(ORDER_SERVO_OFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-11-a7db2a28c25b>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-a7db2a28c25b>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def robot_exec(x0, xT, ds_stab, options):\n",
    "    d = x0.shape[0]\n",
    "    nbSPoint = x0.shape[1]\n",
    "    \n",
    "    if not xT:\n",
    "        xT = np.zeros((d, 1))\n",
    "    \n",
    "    if d != xT.shape[0]:\n",
    "        logger.critical('Error: length x0 should be length xT')\n",
    "        x, xd, t = [], [], []\n",
    "        return\n",
    "    \n",
    "    #################################################\n",
    "    #                   starting values             #\n",
    "    #################################################\n",
    "    x  = np.zeros((d, 0, nbSPoint))\n",
    "    xd = np.zeros((d, 0, nbSPoint))\n",
    "    for i in range(nbSPoint):\n",
    "        x[:, 0, i] = x0[:,i]\n",
    "    xd = np.zeros(x.shape)\n",
    "    \n",
    "    if xT.shape == x0.shape:\n",
    "        XT = xT\n",
    "    else:\n",
    "        XT = np.tile(xT, [1, nbSPoint])\n",
    "    \n",
    "    t = 0\n",
    "    # robot run\n",
    "    i = 0\n",
    "#     while True:\n",
    "#         xd[:,i,:] = np.tile(ds_stab(np.squeeze(x[:,i,:])-XT), [d, 1, nbSPoint])\n",
    "        \n",
    "        \n",
    "#         ### integration ###\n",
    "#         x[:,i+1,:] = x[:,i,:] + xd[:,i,:] * options['dt']\n",
    "#         t[i+1] = t[i]+options['dt']\n",
    "        \n",
    "#         i += 1\n",
    "        \n",
    "#         # check convergence\n",
    "#         if i > 3 and (np.all(np.all(np.all(abs(xd[:,-1-3:-1,:])))))\n",
    "    while np.linalg.norm():\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/olalekan/Documents/LyapunovLearner/ToroboTakahashi/data/state_joint.npy\n"
     ]
    }
   ],
   "source": [
    "filepath = join(expanduser('~'), 'Documents', 'LyapunovLearner', 'ToroboTakahashi', 'data')\n",
    "name = 'state_joint.npy'\n",
    "filename = join(filepath, name)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_joint.npy.bak', 'state_joint.npy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 7) float64\n"
     ]
    }
   ],
   "source": [
    "data = np.load(filename)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(filepath + '/state_joint_pos_only.npy', data[:, 1:8])\n",
    "np.savetxt(filepath + '/state_joint_pos_only.csv', data[:,1:8], delimiter=\" \", newline=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  1.00000005e-03, -1.00000005e-03,\n",
       "         0.00000000e+00, -1.00000005e-03,  0.00000000e+00,\n",
       "        -9.99999978e-03],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -2.00000009e-03,\n",
       "         1.30000003e-02, -1.00000005e-03, -2.00000009e-03,\n",
       "        -1.20000001e-02],\n",
       "       [-9.99999978e-03, -4.10000011e-02, -7.00000022e-03,\n",
       "         4.79999989e-01, -8.60000029e-02,  2.30000000e-02,\n",
       "        -1.30000003e-02],\n",
       "       [ 2.00000009e-03, -1.72000006e-01, -3.79999988e-02,\n",
       "         1.94900000e+00, -3.84999990e-01,  9.25000012e-01,\n",
       "        -1.30000003e-02],\n",
       "       [ 1.00000005e-03, -3.54999989e-01, -1.67999998e-01,\n",
       "         3.47499990e+00, -8.00000012e-01,  2.05500007e+00,\n",
       "        -1.20000001e-02],\n",
       "       [-1.79999992e-02, -5.78999996e-01, -2.89999992e-01,\n",
       "         4.86499977e+00, -1.21899998e+00,  2.71700001e+00,\n",
       "        -1.30000003e-02],\n",
       "       [-7.80000016e-02, -7.65999973e-01, -4.09000009e-01,\n",
       "         6.46199989e+00, -1.60099995e+00,  3.84699988e+00,\n",
       "        -1.20000001e-02],\n",
       "       [-1.22000001e-01, -9.07000005e-01, -5.00999987e-01,\n",
       "         7.76999998e+00, -1.92999995e+00,  4.52600002e+00,\n",
       "        -1.20000001e-02],\n",
       "       [-1.45999998e-01, -1.13199997e+00, -5.64000010e-01,\n",
       "         9.28800011e+00, -2.29699993e+00,  5.48299980e+00,\n",
       "        -1.20000001e-02],\n",
       "       [-1.94000006e-01, -1.26600003e+00, -6.85000002e-01,\n",
       "         1.08360004e+01, -2.71000004e+00,  6.34100008e+00,\n",
       "        -9.99999978e-03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data = np.loadtxt(filepath + '/state_joint_pos_only.csv')\n",
    "pos_data[0:10 , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.001       0.016      ... -0.008       0.008\n",
      "  -0.03      ]\n",
      " [ 0.          0.          0.015      ... -0.009       0.006\n",
      "  -0.03      ]\n",
      " [-0.005      -0.053       0.009      ... -0.126       0.26300001\n",
      "  -0.034     ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "new_data = np.ravel(pos_data)\n",
    "rec_data = new_data.reshape(10001, 7)\n",
    "print(rec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/olalekan/Documents/LyapunovLearner/ToroboTakahashi/data/state_joint.csv'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename.rsplit(sep=\".\")[0] + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(filename.rsplit(sep=\".\")[0] + '.csv', data, delimiter=\",\",  newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '0.000000000000000000e+00,0.000000000000000000e+00,1.000000047497451305e-03,1.600000075995922089e-02,2.000000094994902611e-03,-8.000000379979610443e-03,8.000000379979610443e-03,-2.999999932944774628e-02,1.299999952316284180e-01,1.799999922513961792e-02,0.000000000000000000e+00,-2.989999949932098389e-01,1.000000047497451305e-03,-7.820000052452087402e-01,0.000000000000000000e+00,6.300000101327896118e-02,-1.439999938011169434e-01,-7.999999821186065674e-02,-1.940000057220458984e-01,-0.000000000000000000e+00,-6.400000303983688354e-02,5.499999970197677612e-02,5.235987919149920344e-05,7.700728943405010796e-09,7.799999713897705078e-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-458f605f8a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtxtloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'0x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '0.000000000000000000e+00,0.000000000000000000e+00,1.000000047497451305e-03,1.600000075995922089e-02,2.000000094994902611e-03,-8.000000379979610443e-03,8.000000379979610443e-03,-2.999999932944774628e-02,1.299999952316284180e-01,1.799999922513961792e-02,0.000000000000000000e+00,-2.989999949932098389e-01,1.000000047497451305e-03,-7.820000052452087402e-01,0.000000000000000000e+00,6.300000101327896118e-02,-1.439999938011169434e-01,-7.999999821186065674e-02,-1.940000057220458984e-01,-0.000000000000000000e+00,-6.400000303983688354e-02,5.499999970197677612e-02,5.235987919149920344e-05,7.700728943405010796e-09,7.799999713897705078e-01'"
     ]
    }
   ],
   "source": [
    "txtloader = np.loadtxt(filename.rsplit(sep=\".\")[0] + '.csv')\n",
    "print(txtloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
